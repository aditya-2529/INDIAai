{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sg1iLKbkB0_f",
        "outputId": "8e3f434f-6d30-4c00-9e12-bf64cc0cd267"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to C:\\Users\\ranja/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\ranja/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "import re\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7I2QPrBGbX_Q",
        "outputId": "cb266ebc-4885-44ca-a505-50715fff9c04"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total no. of rows: 372777\n",
            "Field names are:category, sub_category, crimeaditionalinfo\n"
          ]
        }
      ],
      "source": [
        "import csv\n",
        "fields = []\n",
        "rows = []\n",
        "\n",
        "# reading csv file\n",
        "with open('train.csv', 'r') as csvfile:\n",
        "    # creating a csv reader object\n",
        "    csvreader = csv.reader(csvfile)\n",
        "\n",
        "    # extracting field names through first row\n",
        "    fields = next(csvreader)\n",
        "\n",
        "    # extracting each data row one by one\n",
        "    for row in csvreader:\n",
        "        rows.append(row)\n",
        "\n",
        "    # get total number of rows\n",
        "    print(\"Total no. of rows: %d\" % (csvreader.line_num))\n",
        "\n",
        "# printing the field names\n",
        "print('Field names are:' + ', '.join(field for field in fields))\n",
        "category = []\n",
        "sentence = []\n",
        "sub_category = []\n",
        "# printing first 5 rows\n",
        "# print('\\nFirst 5 rows are:\\n')\n",
        "for row in rows:\n",
        "    # parsing each column of a row\n",
        "    for i in range(len(row)):\n",
        "        if i ==0:\n",
        "            category.append(row[i])\n",
        "        if i == 1:\n",
        "            sub_category.append(row[i])\n",
        "        if i == 2:\n",
        "            sentence.append(row[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "2h3U6uzZab1Q"
      },
      "outputs": [],
      "source": [
        "# data = pd.DataFrame({\n",
        "#     'text': [\n",
        "#         \"My credit card was hacked and money was stolen\",\n",
        "#         \"Someone scammed me by pretending to be a bank agent\",\n",
        "#         \"I am a victim of online phishing\",\n",
        "#         \"They took my money through a fake investment scheme\",\n",
        "#         \"Identity theft occurred and my personal data was used\"\n",
        "#     ],\n",
        "#     'category': ['fraud', 'fraud', 'phishing', 'fraud', 'identity theft']\n",
        "# })\n",
        "data = pd.DataFrame({\n",
        "    'text': sentence,\n",
        "    'category': category\n",
        "})\n",
        "\n",
        "# Text Preprocessing Function\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Define a function for text preprocessing\n",
        "def preprocess_text(text):\n",
        "    # Convert text to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove special characters, numbers, and punctuation\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "\n",
        "    # Tokenize the text into words\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # # Remove stopwords and apply stemming\n",
        "    cleaned_tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "    # # Join tokens back into a single string\n",
        "    cleaned_text = ' '.join(cleaned_tokens)\n",
        "\n",
        "    return cleaned_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uYlBwS39aejo",
        "outputId": "8775cca0-0a2b-4374-81fa-4b498c4d11d5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\ranja\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.74\n",
            "Precision: 0.69\n",
            "Recall: 0.74\n",
            "F1-Score: 0.70\n"
          ]
        }
      ],
      "source": [
        "data['cleaned_text'] = data['text'].apply(preprocess_text)\n",
        "\n",
        "# Feature Extraction using TF-IDF\n",
        "tfidf = TfidfVectorizer(max_features=500)\n",
        "X = tfidf.fit_transform(data['cleaned_text']).toarray()\n",
        "y = data['category']\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.9, random_state=42)\n",
        "\n",
        "# Model Development: Logistic Regression\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the Model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred, average='weighted')\n",
        "recall = recall_score(y_test, y_pred, average='weighted')\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall: {recall:.2f}\")\n",
        "print(f\"F1-Score: {f1:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ln7s7O18n9Bh",
        "outputId": "afa7260c-e899-4c9e-a880-fef5a8144e85"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\ranja\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "c:\\Users\\ranja\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "c:\\Users\\ranja\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.7420749279538905\n",
            "Classification Report:\n",
            "                                                      precision    recall  f1-score   support\n",
            "\n",
            "                               Any Other Cyber Crime       0.34      0.24      0.28      2091\n",
            "Child Pornography CPChild Sexual Abuse Material CSAM       0.93      0.20      0.33        69\n",
            "                                Cryptocurrency Crime       0.71      0.05      0.10        96\n",
            "                      Cyber Attack/ Dependent Crimes       1.00      1.00      1.00       765\n",
            "                                     Cyber Terrorism       0.00      0.00      0.00        31\n",
            "      Hacking  Damage to computercomputer system etc       0.38      0.19      0.26       341\n",
            "                            Online Cyber Trafficking       0.00      0.00      0.00        34\n",
            "                              Online Financial Fraud       0.82      0.92      0.87     11471\n",
            "                            Online Gambling  Betting       0.00      0.00      0.00        97\n",
            "               Online and Social Media Related Crime       0.50      0.63      0.56      2453\n",
            "                                          Ransomware       0.00      0.00      0.00        11\n",
            "           RapeGang Rape RGRSexually Abusive Content       1.00      0.90      0.95       565\n",
            "                             Report Unlawful Content       0.00      0.00      0.00         1\n",
            "                               Sexually Explicit Act       0.00      0.00      0.00       322\n",
            "                           Sexually Obscene material       1.00      0.00      0.01       391\n",
            "\n",
            "                                            accuracy                           0.74     18738\n",
            "                                           macro avg       0.45      0.28      0.29     18738\n",
            "                                        weighted avg       0.71      0.74      0.71     18738\n",
            "\n",
            "Confusion Matrix:\n",
            "[[  493     0     0     0     0    35     0  1223     0   340     0     0\n",
            "      0     0     0]\n",
            " [    1    14     0     0     0     0     0    16     0    38     0     0\n",
            "      0     0     0]\n",
            " [   23     0     5     0     0     0     0    62     0     6     0     0\n",
            "      0     0     0]\n",
            " [    0     0     0   765     0     0     0     0     0     0     0     0\n",
            "      0     0     0]\n",
            " [    8     0     0     0     0     1     0    14     0     8     0     0\n",
            "      0     0     0]\n",
            " [   75     0     0     0     0    66     0    66     0   134     0     0\n",
            "      0     0     0]\n",
            " [    4     0     0     0     0     1     0    19     0    10     0     0\n",
            "      0     0     0]\n",
            " [  564     0     2     0     0    36     0 10508     0   361     0     0\n",
            "      0     0     0]\n",
            " [   11     0     0     0     0     0     0    71     0    15     0     0\n",
            "      0     0     0]\n",
            " [  219     0     0     0     0    32     0   660     0  1542     0     0\n",
            "      0     0     0]\n",
            " [    3     0     0     0     0     0     0     2     0     6     0     0\n",
            "      0     0     0]\n",
            " [    0     0     0     0     0     0     0    10     0    44     0   511\n",
            "      0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     1     0     0\n",
            "      0     0     0]\n",
            " [   19     1     0     1     0     2     0    65     0   234     0     0\n",
            "      0     0     0]\n",
            " [   17     0     0     0     0     0     0    48     0   324     0     1\n",
            "      0     0     1]]\n"
          ]
        }
      ],
      "source": [
        "vectorizer = TfidfVectorizer(stop_words='english', max_features=1000)\n",
        "X = vectorizer.fit_transform(data['text'])\n",
        "y = data['category']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Model Training using Naive Bayes Classifier\n",
        "nb_classifier = MultinomialNB()\n",
        "nb_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = nb_classifier.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "classification_rep = classification_report(y_test, y_pred)\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Print evaluation metrics\n",
        "print(f'Accuracy: {accuracy}')\n",
        "print('Classification Report:')\n",
        "print(classification_rep)\n",
        "print('Confusion Matrix:')\n",
        "print(conf_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "aBnBvcgjpSvk",
        "outputId": "69e6f200-7562-4558-a34d-e918801e58bc"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[6], line 21\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Model Development: Initialize and train the SVM classifier\u001b[39;00m\n\u001b[0;32m     20\u001b[0m svm_model \u001b[38;5;241m=\u001b[39m SVC(kernel\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlinear\u001b[39m\u001b[38;5;124m'\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m---> 21\u001b[0m \u001b[43msvm_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_tfidf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Predict on the test data\u001b[39;00m\n\u001b[0;32m     24\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m svm_model\u001b[38;5;241m.\u001b[39mpredict(X_test_tfidf)\n",
            "File \u001b[1;32mc:\\Users\\ranja\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\ranja\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\svm\\_base.py:250\u001b[0m, in \u001b[0;36mBaseLibSVM.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    247\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[LibSVM]\u001b[39m\u001b[38;5;124m\"\u001b[39m, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    249\u001b[0m seed \u001b[38;5;241m=\u001b[39m rnd\u001b[38;5;241m.\u001b[39mrandint(np\u001b[38;5;241m.\u001b[39miinfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mi\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mmax)\n\u001b[1;32m--> 250\u001b[0m \u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msolver_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_seed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m \u001b[38;5;66;03m# see comment on the other call to np.iinfo in this file\u001b[39;00m\n\u001b[0;32m    253\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape_fit_ \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(X, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m (n_samples,)\n",
            "File \u001b[1;32mc:\\Users\\ranja\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\svm\\_base.py:369\u001b[0m, in \u001b[0;36mBaseLibSVM._sparse_fit\u001b[1;34m(self, X, y, sample_weight, solver_type, kernel, random_seed)\u001b[0m\n\u001b[0;32m    355\u001b[0m kernel_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sparse_kernels\u001b[38;5;241m.\u001b[39mindex(kernel)\n\u001b[0;32m    357\u001b[0m libsvm_sparse\u001b[38;5;241m.\u001b[39mset_verbosity_wrap(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose)\n\u001b[0;32m    359\u001b[0m (\n\u001b[0;32m    360\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msupport_,\n\u001b[0;32m    361\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msupport_vectors_,\n\u001b[0;32m    362\u001b[0m     dual_coef_data,\n\u001b[0;32m    363\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintercept_,\n\u001b[0;32m    364\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_support,\n\u001b[0;32m    365\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_probA,\n\u001b[0;32m    366\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_probB,\n\u001b[0;32m    367\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_status_,\n\u001b[0;32m    368\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_iter,\n\u001b[1;32m--> 369\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[43mlibsvm_sparse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlibsvm_sparse_train\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    370\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    371\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    372\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    373\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindptr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    374\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    375\u001b[0m \u001b[43m    \u001b[49m\u001b[43msolver_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    376\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkernel_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    377\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdegree\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    378\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gamma\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    379\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcoef0\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    380\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    381\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mC\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    382\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mclass_weight_\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    383\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    384\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnu\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    385\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcache_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    386\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepsilon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    387\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshrinking\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    388\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprobability\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    389\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    390\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrandom_seed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    391\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    393\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_warn_from_fit_status()\n\u001b[0;32m    395\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclasses_\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
            "File \u001b[1;32m_libsvm_sparse.pyx:218\u001b[0m, in \u001b[0;36msklearn.svm._libsvm_sparse.libsvm_sparse_train\u001b[1;34m()\u001b[0m\n",
            "File \u001b[1;32mc:\\Users\\ranja\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\scipy\\sparse\\_compressed.py:27\u001b[0m, in \u001b[0;36m_cs_matrix.__init__\u001b[1;34m(self, arg1, shape, dtype, copy)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01m_cs_matrix\u001b[39;00m(_data_matrix, _minmax_mixin, IndexMixin):\n\u001b[0;32m     23\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;124;03m    base array/matrix class for compressed row- and column-oriented arrays/matrices\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, arg1, shape\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m     28\u001b[0m         _data_matrix\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, arg1)\n\u001b[0;32m     29\u001b[0m         is_array \u001b[38;5;241m=\u001b[39m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, sparray)\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "# Load the dataset (assuming a CSV file with 'text' and 'label' columns)\n",
        "data = pd.read_csv('train.csv')\n",
        "\n",
        "data['crimeaditionalinfo'] = data['crimeaditionalinfo'].fillna('')\n",
        "# Extract the text and labels\n",
        "X = data['crimeaditionalinfo']\n",
        "y = data['category']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Text Preprocessing: Convert text to TF-IDF features\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "# Model Development: Initialize and train the SVM classifier\n",
        "svm_model = SVC(kernel='linear', random_state=42)\n",
        "svm_model.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Predict on the test data\n",
        "y_pred = svm_model.predict(X_test_tfidf)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred, average='weighted')\n",
        "recall = recall_score(y_test, y_pred, average='weighted')\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall: {recall:.2f}\")\n",
        "print(f\"F1 Score: {f1:.2f}\")\n",
        "\n",
        "# Detailed classification report\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1K_nq3Zt1WIs"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, SpatialDropout1D\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# Load your dataset\n",
        "# Make sure to replace 'your_data.csv' with your actual dataset\n",
        "# and 'text_column' with the column name that contains text\n",
        "# 'label_column' should be the name of the column containing the labels\n",
        "df = pd.read_csv('train.csv')\n",
        "# texts = df['crimeaditionalinfo'].values\n",
        "labels = df['category'].values\n",
        "\n",
        "# Encode labels into numeric values\n",
        "label_encoder = LabelEncoder()\n",
        "labels = label_encoder.fit_transform(labels)\n",
        "labels = to_categorical(labels)\n",
        "\n",
        "# Text preprocessing\n",
        "# Tokenize and convert texts to sequences\n",
        "tokenizer = Tokenizer(num_words=5000, lower=True)\n",
        "tokenizer.fit_on_texts(X)\n",
        "word_index = tokenizer.word_index\n",
        "sequences = tokenizer.texts_to_sequences(X)\n",
        "X = pad_sequences(sequences, maxlen=100)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Model building\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=len(word_index) + 1, output_dim=128, input_length=100))\n",
        "model.add(SpatialDropout1D(0.2))\n",
        "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(labels.shape[1], activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=10, batch_size=64, validation_data=(X_test, y_test), verbose=1)\n",
        "\n",
        "# Evaluate the model\n",
        "y_pred = model.predict(X_test, batch_size=64, verbose=1)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "y_test_classes = np.argmax(y_test, axis=1)\n",
        "\n",
        "# Print accuracy and classification report\n",
        "print(\"Accuracy:\", accuracy_score(y_test_classes, y_pred_classes))\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test_classes, y_pred_classes, target_names=label_encoder.classes_))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "jIbBPHtchrKj"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "# Save the trained model\n",
        "with open('model.pkl', 'wb') as model_file:\n",
        "    pickle.dump(model, model_file)\n",
        "\n",
        "# Save the TF-IDF vectorizer\n",
        "with open('tfidf_vectorizer.pkl', 'wb') as tfidf_file:\n",
        "    pickle.dump(tfidf, tfidf_file)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
